import os
import json
import subprocess
import csv
from codecarbon import EmissionsTracker
from datetime import datetime
import time
import pandas as pd
import shutil
from dotenv import load_dotenv
import plotly.graph_objects as go
from xhtml2pdf import pisa
from jinja2 import Environment, FileSystemLoader, Template
import plotly.io as pio
import plotly.express as px
import plotly.graph_objs as go
import logging
import sys
from plotly.subplots import make_subplots

# Base Directory and Environment Configuration
BASE_DIR = '/app/project'
TEMP_DIR = '/app/'

# Load environment variables
env_path = os.path.join(BASE_DIR, ".env")
if os.path.exists(env_path):
    load_dotenv(dotenv_path=env_path, verbose=True, override=True)
else:
    logging.warning(f"Environment file {env_path} not found. Using default values.")

# Define directories based on BASE_DIR
SOURCE_DIRECTORY = BASE_DIR
GREEN_REFINED_DIRECTORY = os.path.join(SOURCE_DIRECTORY, 'GreenCode')
RESULT_DIR = os.path.join(SOURCE_DIRECTORY, 'Result')
REPORT_DIR = os.path.join(SOURCE_DIRECTORY, 'Report')

# List of files and directories to exclude from processing
EXCLUDED_FILES = [file.strip() for file in os.getenv('EXCLUDED_FILES', '').split(',') if file.strip()]
EXCLUDED_DIRECTORIES = [file.strip() for file in os.getenv('EXCLUDED_DIRECTORIES', '').split(',') if file.strip()]

def is_test_file(file_path):
    """
    Check if a file is a test file based on its name and parent directory.
    """
    file_name = os.path.basename(file_path).lower()
    parent_dir = os.path.basename(os.path.dirname(file_path)).lower()
    return (
        'test' in file_name or
        file_name.startswith('test_') or
        file_name.endswith(('_test.py', 'test.java', 'test.cpp', 'test.cs')) or
        parent_dir in {'test', 'tests'}
    )

def count_lines_of_code(file_path, language="python"):
    """
    Count the number of lines of code in a file, excluding blank lines and comments,
    for multiple programming languages.
    
    Args:
        file_path (str): Path to the file to count lines.
        language (str): Programming language of the file (default: 'python').
    
    Returns:
        int: Number of lines of code.
    """
    # Define comment styles for supported languages
    comment_styles = {
        "python": {"single": "#", "multi_start": ('"""', "'''"), "multi_end": ('"""', "'''")},
        "javascript": {"single": "//", "multi_start": ("/*",), "multi_end": ("*/",)},
        "java": {"single": "//", "multi_start": ("/*",), "multi_end": ("*/",)},
        "c": {"single": "//", "multi_start": ("/*",), "multi_end": ("*/",)},
        "cpp": {"single": "//", "multi_start": ("/*",), "multi_end": ("*/",)},
    }
    
    if language not in comment_styles:
        raise ValueError(f"Unsupported language: {language}")
    
    styles = comment_styles[language]
    
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            lines = file.readlines()
            
        # Initialize counter
        loc = 0
        in_multiline_comment = False
        
        for line in lines:
            # Remove leading/trailing whitespace
            line = line.strip()
            
            # Skip empty lines
            if not line:
                continue
                
            # Handle multiline comments
            if in_multiline_comment:
                if any(line.endswith(end) for end in styles["multi_end"]):
                    in_multiline_comment = False
                continue
            
            if any(line.startswith(start) for start in styles["multi_start"]):
                if not any(line.endswith(end) for end in styles["multi_end"]):
                    in_multiline_comment = True
                continue
                
            # Skip single-line comments
            if line.startswith(styles["single"]):
                continue
            
            loc += 1
        
        return loc
    except Exception as e:
        logging.error(f"Error counting lines of code in {file_path}: {e}")
        return 0

def process_emissions_for_file(tracker, script_path, emissions_csv, file_type, result_dir, test_command):
    if not is_test_file(script_path) or not test_command:
        return

    emissions_data = None
    duration = 0
    test_output = 'Unknown'
    loc = count_lines_of_code(script_path)
    script_name = os.path.basename(script_path)
    solution_dir = os.path.basename(os.path.dirname(script_path))
    is_green_refined = os.path.commonpath([script_path, GREEN_REFINED_DIRECTORY]) == GREEN_REFINED_DIRECTORY

    try:
        with EmissionsTracker() as tracker:
            start_time = time.time()
            test_result = subprocess.run(test_command, capture_output=True, text=True, timeout=20)
            duration = time.time() - start_time
            test_output = 'Pass' if test_result.returncode == 0 else 'Fail'
    except subprocess.TimeoutExpired:
        test_output = 'Timeout'
    except Exception as e:
        logging.error(f"Error processing {script_name}: {e}")
        test_output = 'Error'

    emissions_csv_target = os.path.join(result_dir, 'emissions.csv')
    if os.path.exists('emissions.csv'):
        shutil.move('emissions.csv', emissions_csv_target)

    if os.path.exists(emissions_csv_target) and os.stat(emissions_csv_target).st_size > 0:
        try:
            emissions_df = pd.read_csv(emissions_csv_target)
            if not emissions_df.empty:
                latest_emission = emissions_df.iloc[-1]
                data = [
                    script_name, file_type, datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    f"{latest_emission['emissions'] * 1000:.6f}", f"{duration:.2f}",
                    f"{latest_emission['emissions_rate'] * 1000:.6f}",
                    f"{latest_emission['cpu_power']:.6f}", f"{latest_emission['gpu_power']:.6f}",
                    f"{latest_emission['ram_power']:.6f}", f"{latest_emission['cpu_energy'] * 1000:.6f}",
                    f"{latest_emission['gpu_energy']:.6f}", f"{latest_emission['ram_energy'] * 1000:.6f}",
                    f"{latest_emission['energy_consumed'] * 1000:.6f}", test_output,
                    solution_dir, is_green_refined, loc
                ]
                with open(emissions_csv, 'a', newline='') as f:
                    csv.writer(f).writerow(data)
        except Exception as e:
            logging.error(f"Error writing data for {script_name}: {e}")

def process_files_by_type(base_dir, emissions_csv, result_dir, file_extension, excluded_files, excluded_dirs, test_command_generator):
    files = []
    for root, dirs, file_list in os.walk(base_dir):
        dirs[:] = [d for d in dirs if d not in excluded_dirs]
        file_list = [
            f for f in file_list
            if f.endswith(file_extension) and f not in excluded_files and is_test_file(os.path.join(root, f))
        ]

        for script in file_list:
            script_path = os.path.join(root, script)
            test_command = test_command_generator(script_path)
            if test_command:
                files.append((script_path, test_command))

    for script_path, test_command in files:
        process_emissions_for_file(
            tracker=None,
            script_path=script_path,
            emissions_csv=emissions_csv,
            file_type=file_extension,
            result_dir=result_dir,
            test_command=test_command
        )
# Generate test commands for each language
def get_python_test_command(script_path):
    return [os.getenv('PYTEST_PATH', 'pytest'), script_path] if 'test' in script_path.lower() else None

# --------------- Improved version of get_java_test_command ----------------
def get_java_test_command(script_path):
    maven_path = os.getenv('MAVEN_PATH', 'mvn')
    test_name = os.path.splitext(os.path.basename(script_path))[0] + 'Test'
    return [maven_path, '-Dtest=' + test_name, 'test'] if 'test' in script_path.lower() else None
# ------------------------------------------------------------------------  

def get_cpp_test_command(script_path):
    if 'test' in script_path.lower():
        # Assuming a standard project structure
        test_file_name = os.path.basename(script_path).replace('.cpp', '_test.cpp')
        test_dir = os.path.join(os.path.dirname(script_path), 'test')
        test_file_path = os.path.join(test_dir, test_file_name)

        # Verify test file exists
        if not os.path.exists(test_file_path):
            logging.info(f"Warning: Test file {test_file_path} does not exist")
            return None
        
        # Create a temporary build directory
        build_dir = os.path.join(test_dir, 'build')
        os.makedirs(build_dir, exist_ok=True)
        
        # CMake command to configure the project
        cmake_path = os.getenv('GTEST_CMAKE_PATH', 'cmake')
        cmake_config_command = [
            cmake_path,
            f'-S{test_dir}',
            f'-B{build_dir}',
            '-DCMAKE_PREFIX_PATH=/usr/local',
            '-G', 'Unix Makefiles'
        ]
        
        # CMake build command
        cmake_build_command = [cmake_path, '--build', build_dir]
        
        # Test run command
        test_executable = os.path.join(build_dir, os.path.splitext(test_file_name)[0])        
        if not os.path.exists(test_executable):
            logging.warning(f"Test executable {test_executable} not found")
            return None
        
        return cmake_config_command + cmake_build_command + [test_executable]    
    return None

# ------------------ improved version of get_cs_test_command --------------
def get_cs_test_command(script_path):
    test_project = os.path.splitext(os.path.basename(script_path))[0]
    test_command = ['dotnet', 'test', test_project] if 'test' in script_path.lower() else None
    return test_command
# --------------------------------------------------------------------------


# Refactored process_folder function
def process_folder(base_dir, emissions_csv, result_dir, suffix, excluded_dirs):
    os.makedirs(result_dir, exist_ok=True)
    
    if not os.path.exists(emissions_csv):
        with open(emissions_csv, 'w', newline='') as f:
            csv.writer(f).writerow([
                "Application name", "File Type", "Timestamp", "Emissions (gCO2eq)",
                "Duration", "emissions_rate", "CPU Power (KWh)", "GPU Power (KWh)", "RAM Power (KWh)",
                "CPU Energy (Wh)", "GPU Energy (KWh)", "RAM Energy (Wh)", "Energy Consumed (Wh)", "Test Results",
                "solution dir", "Is Green Refined", "Lines of Code"
            ])

    # Adjust excluded directories when processing GreenCode
    adjusted_excluded_dirs = [
        d for d in excluded_dirs 
        if not (base_dir == GREEN_REFINED_DIRECTORY and d == 'GreenCode')
    ]

    extensions = {
        '.py': get_python_test_command,
        '.java': get_java_test_command,
        '.cpp': get_cpp_test_command,
        '.cs': get_cs_test_command
    }

    for ext, cmd_generator in extensions.items():
        process_files_by_type(
            base_dir=base_dir,
            emissions_csv=emissions_csv,
            result_dir=result_dir,
            file_extension=ext,
            excluded_files=EXCLUDED_FILES,
            excluded_dirs=adjusted_excluded_dirs,  # Use adjusted list
            test_command_generator=cmd_generator
        )

# Execute processing
process_folder(
    base_dir=SOURCE_DIRECTORY,
    emissions_csv=os.path.join(RESULT_DIR, 'main_before_emissions_data.csv'),
    result_dir=RESULT_DIR,
    suffix='before',
    excluded_dirs=EXCLUDED_DIRECTORIES
)

process_folder(
    base_dir=GREEN_REFINED_DIRECTORY,
    emissions_csv=os.path.join(RESULT_DIR, 'main_after_emissions_data.csv'),
    result_dir=RESULT_DIR,
    suffix='after',
    excluded_dirs=EXCLUDED_DIRECTORIES
)

logging.info("Emissions tracking completed successfully.")

# Compare emissions logic
def compare_emissions():
    # Load environment variables again (if needed)
    load_dotenv(dotenv_path=env_path, verbose=True, override=True)

    # Define paths to the before and after CSV files
    result_source_dir = os.path.join(SOURCE_DIRECTORY, 'Result', 'main_before_emissions_data.csv')
    result_green_refined_dir = os.path.join(SOURCE_DIRECTORY, 'Result', 'main_after_emissions_data.csv')

    # Check if both CSV files exist
    if not os.path.isfile(result_source_dir):
        logging.info(f"Source emissions data file not found: {result_source_dir}")
        return
    if not os.path.isfile(result_green_refined_dir):
        logging.info(f"Refined emissions data file not found: {result_green_refined_dir}")
        return

    # Read CSV files
    emissions_df = pd.read_csv(result_source_dir)
    emissions_after_df = pd.read_csv(result_green_refined_dir)

    # Merge dataframes on common columns
    try:
        merged_df = emissions_df.merge(
            emissions_after_df,
            on=["Application name", "File Type"],
            suffixes=('_before', '_after')
        )
    except KeyError as e:
        logging.info(f"Merge failed due to missing columns: {e}")
        return

    # Calculate the difference in emissions and determine the result
    merged_df['final emission'] = merged_df['Emissions (gCO2eq)_before'] - merged_df['Emissions (gCO2eq)_after']
    merged_df['Result'] = merged_df['final emission'].apply(lambda x: 'Improved' if x > 0 else 'Need improvement')

    # Select and rename columns
    result_df = merged_df[[
        "Application name",
        "File Type",
        "Timestamp_before",
        "Timestamp_after",
        "Emissions (gCO2eq)_before",
        "Emissions (gCO2eq)_after",
        "final emission",
        "Result"
    ]]
    result_df.columns = [
        "Application name",
        "File Type",
        "Timestamp (Before)",
        "Timestamp (After)",
        "Before",
        "After",
        "Final Emission",
        "Result"
    ]

    # Create 'Result' folder if it doesn't exist
    if not os.path.exists(RESULT_DIR):
        os.makedirs(RESULT_DIR)
        logging.info(f"Directory '{RESULT_DIR}' created successfully!")
    else:
        logging.info(f"Directory '{RESULT_DIR}' already exists.")

    # Write to new CSV file
    result_file_path = os.path.join(RESULT_DIR, "comparison_results.csv")
    result_df.to_csv(result_file_path, index=False)

    logging.info(f"Comparison results saved to {result_file_path}")

# Call the compare_emissions function
compare_emissions()

def prepare_detailed_data(result_dir):
    comparison_csv = os.path.join(result_dir, "comparison_results.csv")
    before_csv = os.path.join(result_dir, 'main_before_emissions_data.csv')
    after_csv = os.path.join(result_dir, 'main_after_emissions_data.csv')
    
    # Read CSV files
    comparison_df = pd.read_csv(comparison_csv)
    before_df = pd.read_csv(before_csv)
    after_df = pd.read_csv(after_csv)
    
    # Merge before and after data
    merged_before = before_df[['Application name', 'File Type', 'Duration', 'Emissions (gCO2eq)', 'Energy Consumed (Wh)', 'solution dir']]
    merged_after = after_df[['Application name', 'File Type', 'Duration', 'Emissions (gCO2eq)', 'Energy Consumed (Wh)', 'solution dir']]
    
    # Group by 'solution dir'
    solution_dirs = sorted(comparison_df['Application name'].unique())  # Adjust as needed
    
    # Get unique solution directories
    solution_dirs = sorted(set(before_df['solution dir']).union(after_df['solution dir']))
    
    # Prepare data for each solution dir
    detailed_data = {}
    for dir in solution_dirs:
        before_details = merged_before[merged_before['solution dir'] == dir].to_dict(orient='records')
        after_details = merged_after[merged_after['solution dir'] == dir].to_dict(orient='records')
        detailed_data[dir] = {
            'before': before_details,
            'after': after_details
        }
    
    return solution_dirs, detailed_data

solution_dirs, detailed_data = prepare_detailed_data(RESULT_DIR)

def generate_html_report(result_dir, solution_dirs, detailed_data):
    # Initialize Jinja2 environment
    env = Environment(loader=FileSystemLoader(TEMP_DIR))
    template_path = 'report_template.html'
    last_run_template_path = 'last_run_report_template.html'
    details_template_path = 'details_template.html'
    last_run_details_template_path = 'last_run_details_template.html'
    details_server_template_path = 'details_server_template.html'
    recommendations_template_path = 'recommendations_template.html'

    solution_dirs, detailed_data = prepare_detailed_data(result_dir)


    # Check if the templates exist
    for path in [details_template_path, template_path, last_run_details_template_path, 
                 last_run_template_path, details_server_template_path, recommendations_template_path]:
        if not os.path.isfile(os.path.join(TEMP_DIR, path)):
            logging.error(f"Template file not found: {path}")
            return

    # Load the templates
    try:
        template = env.get_template(template_path)
        details_template = env.get_template(details_template_path)
        lastrun_template = env.get_template(last_run_template_path)
        lastrun_details_template = env.get_template(last_run_details_template_path)
        details_server_template = env.get_template(details_server_template_path)
        recommendations_template = env.get_template(recommendations_template_path)
    except Exception as e:
        logging.error(f"Failed to load templates: {e}")
        return

    # CSV paths
    csv_files = {
        'before': os.path.join(result_dir, 'main_before_emissions_data.csv'),
        'after': os.path.join(result_dir, 'main_after_emissions_data.csv'),
        'comparison': os.path.join(result_dir, 'comparison_results.csv'),
        'server': os.path.join(result_dir, 'server_data.csv'),
        'mul_server': os.path.join(result_dir, 'multiple_server_data.csv'),
        'recommendations': os.path.join(result_dir, 'modification_overview.csv'),
        'final_overview': os.path.join(result_dir, 'final_overview.csv')
    }

    # Load available CSVs
    dfs = {}
    for name, path in csv_files.items():
        if os.path.exists(path):
            try:
                dfs[name] = pd.read_csv(path)
                logging.info(f"Loaded CSV: {path}")
            except Exception as e:
                logging.error(f"Error reading {path}: {e}")
                dfs[name] = None
        else:
            logging.warning(f"CSV not found: {path}")
            dfs[name] = None

    # Extract DataFrames
    before_df = dfs.get('before')
    after_df = dfs.get('after')
    comparison_df = dfs.get('comparison')
    server_df = dfs.get('server')
    mul_server_df = dfs.get('mul_server')
    recommendations_df = dfs.get('recommendations')
    final_overview_df = dfs.get('final_overview')

    # Initialize default values for template variables
    template_vars = {
        'total_before': 0.0,
        'total_after': 0.0,
        'energy_table_html': "<p>Data unavailable</p>",
        'emissions_table_html': "<p>Data unavailable</p>",
        'total_emissions_before': 0.0,
        'total_emissions_after': 0.0,
        'last_run_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'server_os_type_fig': "<p>OS type data unavailable</p>",
        'server_os_version_fig': "<p>OS version data unavailable</p>",
        'unique_hosts': 0,
        'average_co2_emission': 0.0,
        'average_energy_consumption': 0.0,
        'average_cpu_usage': 0.0,
        'average_ram_usage': 0.0,
        'average_disk_usage': 0.0,
        'average_network_usage': 0.0,
        'average_disk_read': 0.0,
        'average_disk_write': 0.0,
        'cpu_usage_data': [],
        'ram_usage_data': [],
        'disk_usage_data': [],
        'network_usage_data': [],
        'disk_read_data': [],
        'disk_write_data': [],
        'max_network': 0.0,
        'critical_servers': None,
        'div_combined_graph': "<p>Energy consumption data unavailable</p>",
        'div_emissions_combined_graph': "<p>Emissions data unavailable</p>",
        'div_pie_chart_non_embedded': "<p>Non-embedded code data unavailable</p>",
        'div_pie_chart_embedded': "<p>Embedded code data unavailable</p>",
        'unique_servers': [],
        'critical_server_count': 0,
        'total_last_run_co2': 0.0,
        'total_last_run_power': 0.0,
        'formatted_timestamp': "No data",
        'final_overview_data': None,
        'fresh_details': None,
        'total_files_modified_last_run': 0,
        'total_loc_converted_last_run': 0,
        'recommendations_details': None,
        'unique_dates': [],
        'server_details': []
    }

    # Process server data if available
    if server_df is not None:
        try:
            unique_hosts = server_df['IP address'].nunique()
            template_vars['unique_hosts'] = unique_hosts
            template_vars['average_co2_emission'] = round(server_df['CO2 emission (Metric Tons)'].sum() / unique_hosts, 2)
            template_vars['average_energy_consumption'] = round(server_df['Energy consumption (KWH)'].sum() / unique_hosts, 2)
            template_vars['average_cpu_usage'] = round(server_df['CPU usage (%)'].mean(), 2)
            template_vars['average_ram_usage'] = round(server_df['RAM usage (%)'].mean(), 2)
            template_vars['average_disk_usage'] = round(server_df['Disk usage (%)'].mean(), 2)
            template_vars['average_network_usage'] = round(server_df['Network usage (bytes)'].mean(), 2)
            template_vars['cpu_usage_data'] = server_df['CPU usage (%)'].tail(10).tolist()
            template_vars['ram_usage_data'] = server_df['RAM usage (%)'].tail(10).tolist()
            template_vars['disk_usage_data'] = server_df['Disk usage (%)'].tail(10).tolist()
            template_vars['network_usage_data'] = server_df['Network usage (bytes)'].tail(10).tolist()
        except KeyError as e:
            logging.error(f"Missing column in server data: {e}")

    # Process multi-server data if available
    if mul_server_df is not None:
        try:
            template_vars['average_disk_read'] = round(mul_server_df['disk_read_bytes'].mean(), 2)
            template_vars['average_disk_write'] = round(mul_server_df['disk_write_bytes'].mean(), 2)
            template_vars['disk_read_data'] = mul_server_df['disk_read_bytes'].tail(10).tolist()
            template_vars['disk_write_data'] = mul_server_df['disk_write_bytes'].tail(10).tolist()
            template_vars['max_network'] = max(template_vars['network_usage_data'])

            critical_servers = mul_server_df[mul_server_df['total_co2'] > 0.000001][['hostname', 'total_co2']]
            critical_servers = critical_servers.sort_values(by='total_co2', ascending=False).drop_duplicates(subset='hostname')
            template_vars['critical_server_count'] = len(critical_servers)
            template_vars['critical_servers'] = critical_servers.to_dict(orient='records') if not critical_servers.empty else None

            # Convert 'timestamp' column to datetime
            mul_server_df['timestamp'] = pd.to_datetime(
                mul_server_df['timestamp'], 
                format='%Y-%m-%d %H:%M:%S.%f', 
                errors='coerce'  # Force invalid dates to NaT
            )
            # Drop rows with NaT in 'timestamp'
            mul_server_df = mul_server_df.dropna(subset=['timestamp'])
            
            # Get latest timestamp (skip if DataFrame is empty)
            if not mul_server_df.empty:
                latest_timestamp = mul_server_df['timestamp'].max()
                # Check for NaT before formatting
                if pd.notna(latest_timestamp):
                    template_vars['formatted_timestamp'] = latest_timestamp.strftime('%d-%m-%Y %H:%M:%S')
                else:
                    template_vars['formatted_timestamp'] = "No valid timestamp found"
            else:
                template_vars['formatted_timestamp'] = "No timestamp data available"

            # mul_server_df['timestamp'] = pd.to_datetime(mul_server_df['timestamp'], format='%Y-%m-%d %H:%M:%S.%f', errors='coerce')
            # mul_server_df = mul_server_df.dropna(subset=['timestamp'])
            # latest_timestamp = mul_server_df['timestamp'].max()
            last_run_records = mul_server_df[mul_server_df['timestamp'] == latest_timestamp]
            
            # Format the total_last_run_co2 to avoid scientific notation
            total_last_run_co2 = last_run_records['total_co2'].sum()
            total_last_run_power = last_run_records['total_power'].sum()
            template_vars['total_last_run_co2'] = f"{total_last_run_co2:.6f}"  # Format to 6 decimal places
            template_vars['total_last_run_power'] = f"{total_last_run_power:.6f}"  # Format to 6 decimal places
            # template_vars['total_last_run_power'] = last_run_records['total_power'].sum()
            
            # template_vars['formatted_timestamp'] = latest_timestamp.strftime('%d-%m-%Y %H:%M:%S')

            # Count of unique servers by 'os_type' and 'os_version'
            os_type_counts = mul_server_df.groupby(['os_type', 'hostname']).size().reset_index(name='count').groupby('os_type').agg({'hostname': 'count'}).rename(columns={'hostname': 'count'})
            os_version_counts = mul_server_df.groupby(['os_type', 'os_version', 'hostname']).size().reset_index(name='count').groupby(['os_type', 'os_version']).agg({'hostname': 'count'}).reset_index().rename(columns={'hostname': 'count'})
            color_palette = px.colors.qualitative.Pastel

            # Plot 1: Bar graph for 'os_type'
            fig1 = go.Figure(data=[go.Bar(x=os_type_counts.index, y=os_type_counts['count'], text=os_type_counts['count'], textposition='inside', textfont=dict(size=16), marker=dict(color=color_palette[:len(os_type_counts)]))])
            fig1.update_layout(title="Count of Unique Servers by OS Type", xaxis_title="OS Type", yaxis_title="Count of Unique Servers", template="plotly_white", width=600, height=300)
            template_vars['server_os_type_fig'] = pio.to_html(fig1, include_plotlyjs=False, full_html=False)

            # Plot 2: Bar graph for 'os_version' grouped by 'os_type'
            fig2 = go.Figure()
            for i, os_type in enumerate(os_version_counts['os_type'].unique()): os_data = os_version_counts[os_version_counts['os_type'] == os_type]; fig2.add_trace(go.Bar(x=os_data['os_version'], y=os_data['count'], name=os_type, text=os_data['count'], textposition='inside', textfont=dict(size=16), marker=dict(color=color_palette[i])))
            fig2.update_layout(title="Count of Unique Servers by OS Version (Grouped by OS Type)", xaxis_title="OS Version", yaxis_title="Count of Unique Servers", barmode='stack', template="plotly_white", width=600, height=400, xaxis_tickangle=-45, xaxis=dict(tickvals=os_version_counts['os_version'], ticktext=[os.replace(' ', '<br>') for os in os_version_counts['os_version']]), legend=dict(orientation="h", yanchor="top", y=1.20, xanchor="center", x=0.5, font=dict(size=12)), margin=dict(b=100))
            template_vars['server_os_version_fig'] = pio.to_html(fig2, include_plotlyjs=False, full_html=False)

            df = mul_server_df.copy()
            numeric_columns = ['cpu_percent', 'ram_total', 'ram_used', 'ram_percent', 'disk_read_bytes', 
                              'disk_write_bytes', 'total_power', 'cpu_power', 'ram_power', 'disk_base_power', 
                              'disk_io_power', 'total_co2', 'cpu_co2', 'ram_co2', 'disk_base_co2', 
                              'disk_io_co2', 'co2_factor', 'storage_device_count']
            string_columns = ['hostname', 'os_version', 'os_type', 'region', 'storage_devices', 'timestamp']
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
            for col in string_columns:
                if col in df.columns:
                    df[col] = df[col].astype(str)
            for col in numeric_columns + string_columns:
                if col not in df.columns:
                    df[col] = 0 if col in numeric_columns else 'N/A'
            template_vars['server_details'] = df.to_dict(orient='records')
            template_vars['unique_servers'] = df['hostname'].unique().tolist()
        except KeyError as e:
            logging.error(f"Missing column in multi-server data: {e}")

    # Process recommendations data if available
    if recommendations_df is not None:
        try:
            recommendations_df['Modification Timestamp'] = pd.to_datetime(recommendations_df['Modification Timestamp'], errors='coerce')
            recommendations_df = recommendations_df.dropna(subset=['Modification Timestamp'])
            unique_dates = recommendations_df['Modification Timestamp'].dt.date.unique().tolist()
            recommendations_details = {}
            for date in unique_dates:
                date_records = recommendations_df[recommendations_df['Modification Timestamp'].dt.date == date]
                recommendations_details[str(date)] = date_records.assign(
                    **{"Modification Timestamp": date_records['Modification Timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')}
                ).to_dict(orient='records')
            template_vars['recommendations_details'] = recommendations_details
            template_vars['unique_dates'] = unique_dates
        except KeyError as e:
            logging.error(f"Missing column in recommendations data: {e}")

    # Process final_overview data if available
    if final_overview_df is not None:
        try:
            # Extract data from final_overview.csv
            final_overview_data = {
                'fresh_details': {
                    'total_files_modified_last_run': final_overview_df.loc[
                        final_overview_df['Metric'] == 'Total Files Modified (Last run)', 'Value'].values[0],
                    'total_loc_converted_last_run': final_overview_df.loc[
                        final_overview_df['Metric'] == 'Total LOC Converted (Last run)', 'Value'].values[0],
                    'total_time_minutes_last_run': final_overview_df.loc[
                        final_overview_df['Metric'] == 'Total Time (minutes) (Last run)', 'Value'].values[0],
                },
                'historical_overview': {
                    'total_files_modified': final_overview_df.loc[
                        final_overview_df['Metric'] == 'Total Files Modified', 'Value'].values[0],
                    'total_loc_converted': final_overview_df.loc[
                        final_overview_df['Metric'] == 'Total LOC Converted', 'Value'].values[0],
                    'total_time_minutes': final_overview_df.loc[
                        final_overview_df['Metric'] == 'Total Time (minutes)', 'Value'].values[0],
                }
            }
            
            # Dynamically add file type information
            file_type_last_run = final_overview_df[final_overview_df['Metric'].str.contains(r'\.\w+ Files \(Last run\)')]
            if not file_type_last_run.empty:
                final_overview_data['fresh_details']['file_types_last_run'] = {
                    row['Metric'].split()[0]: row['Value'] 
                    for _, row in file_type_last_run.iterrows()
                }
            
            file_types = final_overview_df[final_overview_df['Metric'].str.contains(r'\.\w+ Files$')]
            if not file_types.empty:
                final_overview_data['historical_overview']['file_types'] = {
                    row['Metric'].split()[0]: row['Value'] 
                    for _, row in file_types.iterrows()
                }
            
            template_vars['final_overview_data'] = final_overview_data
        except Exception as e:
            logging.error(f"Error processing final_overview.csv: {e}")
            # Set default values if final_overview.csv is missing or has incorrect data
            template_vars['final_overview_data'] = {
                'fresh_details': {
                    'total_files_modified_last_run': 0,
                    'total_loc_converted_last_run': 0,
                    'total_time_minutes_last_run': 0,
                    'file_types_last_run': {}
                },
                'historical_overview': {
                    'total_files_modified': 0,
                    'total_loc_converted': 0,
                    'total_time_minutes': 0,
                    'file_types': {}
                }
            }
    else:
        logging.warning("final_overview.csv not found or could not be loaded.")
        # Set default values if final_overview.csv is missing or could not be loaded
        template_vars['final_overview_data'] = {
            'fresh_details': {
                'total_files_modified_last_run': 0,
                'total_loc_converted_last_run': 0,
                'total_time_minutes_last_run': 0,
                'file_types_last_run': {}
            },
            'historical_overview': {
                'total_files_modified': 0,
                'total_loc_converted': 0,
                'total_time_minutes': 0,
                'file_types': {}
            }
        }

    # Process before and after data if available
    if before_df is not None and after_df is not None:
        try:
            def is_test_application(app_name):
                return 'test' in str(app_name).lower()

            template_vars['total_before'] = before_df[before_df['Application name'].apply(is_test_application)]['Energy Consumed (Wh)'].astype(float).sum()
            template_vars['total_after'] = after_df[after_df['Application name'].apply(is_test_application)]['Energy Consumed (Wh)'].astype(float).sum()

            if not before_df.empty:
                latest_before_df = before_df.loc[[before_df['Timestamp'].idxmax()]]
                template_vars['latest_before_details'] = [latest_before_df[['Application name', 'File Type', 'Duration', 'Emissions (gCO2eq)', 'Energy Consumed (Wh)', 'solution dir']].to_dict()]
                template_vars['latest_total_before'] = latest_before_df[latest_before_df['Application name'].apply(is_test_application)]['Energy Consumed (Wh)'].astype(float).sum()
            else:
                template_vars['latest_before_details'] = []
                template_vars['latest_total_before'] = 0.0

            if not after_df.empty:
                latest_after_df = after_df.loc[[after_df['Timestamp'].idxmax()]]
                template_vars['latest_after_details'] = [latest_after_df[['Application name', 'File Type', 'Duration', 'Emissions (gCO2eq)', 'Energy Consumed (Wh)', 'solution dir']].to_dict()]
                template_vars['latest_total_after'] = latest_after_df[latest_after_df['Application name'].apply(is_test_application)]['Energy Consumed (Wh)'].astype(float).sum()
            else:
                template_vars['latest_after_details'] = []
                template_vars['latest_total_after'] = 0.0

            template_vars['before_details'] = before_df[['Application name', 'File Type', 'Duration', 'Emissions (gCO2eq)', 'Energy Consumed (Wh)', 'solution dir']].to_dict(orient='records')
            template_vars['after_details'] = after_df[['Application name', 'File Type', 'Duration', 'Emissions (gCO2eq)', 'Energy Consumed (Wh)', 'solution dir']].to_dict(orient='records')

            if comparison_df is not None:
                template_vars['total_emissions_before'] = comparison_df[comparison_df['Application name'].apply(is_test_application)]['Before'].astype(float).sum()
                template_vars['total_emissions_after'] = comparison_df[comparison_df['Application name'].apply(is_test_application)]['After'].astype(float).sum()
                template_vars['latest_total_emissions_before'] = latest_before_df[latest_before_df['Application name'].apply(is_test_application)]['Emissions (gCO2eq)'].astype(float).sum()
                template_vars['latest_total_emissions_after'] = latest_after_df[latest_after_df['Application name'].apply(is_test_application)]['Emissions (gCO2eq)'].astype(float).sum()

            # Generate energy consumption graph
            before_file_type = before_df.groupby('solution dir')['Energy Consumed (Wh)'].sum().reset_index()
            after_file_type = after_df.groupby('solution dir')['Energy Consumed (Wh)'].sum().reset_index()
            before_file_type_sorted = before_file_type.sort_values('Energy Consumed (Wh)', ascending=False)
            after_file_type_sorted = after_file_type.sort_values('Energy Consumed (Wh)', ascending=False)
            unique_solution_dirs = sorted(set(before_file_type_sorted['solution dir']).union(after_file_type_sorted['solution dir']))
            colors = px.colors.qualitative.Pastel
            color_mapping = {solution_dir: colors[i % len(colors)] for i, solution_dir in enumerate(before_file_type_sorted['solution dir'].unique())}

            fig = go.Figure()
            for i, (_, row) in enumerate(before_file_type_sorted.iterrows()):
                fig.add_trace(go.Bar(x=[row['solution dir']], y=[row['Energy Consumed (Wh)']], name='Before', 
                                     marker=dict(color=color_mapping.get(row['solution dir'], 'grey'), opacity=0.9), 
                                     offsetgroup=0, showlegend=True if i == 0 else False))
            for i, (_, row) in enumerate(after_file_type_sorted.iterrows()):
                fig.add_trace(go.Bar(x=[row['solution dir']], y=[row['Energy Consumed (Wh)']], name='After', 
                                     marker=dict(color=color_mapping.get(row['solution dir'], 'grey'), opacity=0.6, 
                                                 pattern=dict(shape="/", solidity=0.7)), offsetgroup=1, 
                                     showlegend=True if i == 0 else False))
            fig.update_layout(title={'text': 'Source Code Directory Level Energy Consumption (Wh) - Before vs After Optimization', 
                                    'y': 0.95, 'x': 0.5, 'xanchor': 'center', 'yanchor': 'top'}, 
                             xaxis_title='Solution Dir', yaxis_title='Energy Consumed (Wh)', barmode='group', 
                             xaxis=dict(tickangle=45, tickformat=".6f"), yaxis=dict(range=[0, max(
                                 before_file_type_sorted['Energy Consumed (Wh)'].max(), 
                                 after_file_type_sorted['Energy Consumed (Wh)'].max()) * 1.1], tickformat=".6f"), 
                             margin=dict(l=50, r=50, t=100, b=120), showlegend=False, width=700, height=400, 
                             plot_bgcolor='white', paper_bgcolor='white')
            fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey')
            template_vars['div_combined_graph'] = pio.to_html(fig, include_plotlyjs=False, full_html=False)

            # Generate emissions graph
            before_gco2eq = before_df.groupby('solution dir')['Emissions (gCO2eq)'].sum().reset_index()
            after_gco2eq = after_df.groupby('solution dir')['Emissions (gCO2eq)'].sum().reset_index()
            before_gco2eq_sorted = before_gco2eq.sort_values('Emissions (gCO2eq)', ascending=False)
            after_gco2eq_sorted = after_gco2eq.sort_values('Emissions (gCO2eq)', ascending=False)

            fig = go.Figure()
            for i, (_, row) in enumerate(before_gco2eq_sorted.iterrows()):
                fig.add_trace(go.Bar(x=[row['solution dir']], y=[row['Emissions (gCO2eq)']], name='Before',
                                     marker=dict(color=color_mapping.get(row['solution dir'], 'grey'), opacity=0.9),
                                     offsetgroup=0, showlegend=True if i == 0 else False))
            for i, (_, row) in enumerate(after_gco2eq_sorted.iterrows()):
                fig.add_trace(go.Bar(x=[row['solution dir']], y=[row['Emissions (gCO2eq)']], name='After',
                                     marker=dict(color=color_mapping.get(row['solution dir'], 'grey'), opacity=0.6,
                                                 pattern=dict(shape="/", solidity=0.7)), offsetgroup=1,
                                     showlegend=True if i == 0 else False))
            fig.update_layout(title={'text': 'Source Code Directory Level Emissions (gCO2eq) - Before vs After Optimization',
                                    'y': 0.95, 'x': 0.5, 'xanchor': 'center', 'yanchor': 'top'},
                             xaxis_title='Solution Dir', yaxis_title='Emissions (gCO2eq)', barmode='group',
                             xaxis=dict(tickangle=45, tickformat=".6f"), yaxis=dict(range=[0, max(
                                 before_gco2eq_sorted['Emissions (gCO2eq)'].max(),
                                 after_gco2eq_sorted['Emissions (gCO2eq)'].max()) * 1.1], tickformat=".6f"),
                             margin=dict(l=50, r=50, t=100, b=120), showlegend=False, width=700, height=400,
                             plot_bgcolor='white', paper_bgcolor='white')
            fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey')
            template_vars['div_emissions_combined_graph'] = pio.to_html(fig, include_plotlyjs=False, full_html=False)

            # Generate top five tables
            top_five_energy_before = before_df.sort_values('Energy Consumed (Wh)', ascending=False).head(5)[
                ['Application name', 'Timestamp', 'Energy Consumed (Wh)']]
            top_five_energy_before.rename(columns={'Application name': 'File Name', 'Timestamp': 'Timestamp',
                                                  'Energy Consumed (Wh)': 'Energy Consumed (Wh)'}, inplace=True)
            template_vars['energy_table_html'] = top_five_energy_before.to_html(index=False, classes='table', border=0,
                                                                               float_format=lambda x: f"{x:.6f}")

            top_five_emissions_before = before_df.sort_values('Emissions (gCO2eq)', ascending=False).head(5)[
                ['Application name', 'Timestamp', 'Emissions (gCO2eq)']]
            top_five_emissions_before.rename(columns={'Application name': 'File Name', 'Timestamp': 'Timestamp',
                                                     'Emissions (gCO2eq)': 'Emissions (gCO2eq)'}, inplace=True)
            template_vars['emissions_table_html'] = top_five_emissions_before.to_html(index=False, classes='table',
                                                                                     border=0, float_format=lambda x: f"{x:.6f}")

            # Generate pie charts for embedded and non-embedded code
            embedded_types = ['.html', '.css', '.xml', '.php', '.ts']
            non_embedded_types = ['.py', '.java', '.cpp', '.rb']
            embedded_df = comparison_df[comparison_df['File Type'].isin(embedded_types)]
            non_embedded_df = comparison_df[comparison_df['File Type'].isin(non_embedded_types)]

            if not embedded_df.empty:
                total_embedded_before = embedded_df['Before'].astype(float).sum()
                total_embedded_after = embedded_df['After'].astype(float).sum()
                reduction_percentage = ((total_embedded_before - total_embedded_after) / total_embedded_before * 100)
                fig = go.Figure(data=[go.Pie(values=[total_embedded_before, total_embedded_after],
                                             labels=['Before', 'After'], marker=dict(colors=['#FF6B6B', '#4ECDC4']),
                                             textinfo='label+value', textposition='outside',
                                             texttemplate='%{label}<br>%{value:.6f} gCO2eq', hole=0.7,
                                             direction='clockwise', showlegend=False)])
                fig.update_layout(title=dict(text='Embedded Code Emissions (gCO2eq)<br>[".html",".css",".xml",".php",".ts"]',
                                             y=0.95, x=0.5, xanchor='center', yanchor='top', font=dict(size=18)),
                                 annotations=[dict(text=f"↓{reduction_percentage:.1f}%", x=0.5, y=0.5,
                                                   font=dict(size=24, color='green'), showarrow=False),
                                             dict(text="Reduction", x=0.5, y=0.42, font=dict(size=14, color='green'),
                                                  showarrow=False)], width=600, height=400, paper_bgcolor='white',
                                 plot_bgcolor='white', showlegend=False)
                template_vars['div_pie_chart_embedded'] = pio.to_html(fig, include_plotlyjs=False, full_html=False)

            if not non_embedded_df.empty:
                total_non_embedded_before = non_embedded_df['Before'].astype(float).sum()
                total_non_embedded_after = non_embedded_df['After'].astype(float).sum()
                reduction_percentage = ((total_non_embedded_before - total_non_embedded_after) / total_non_embedded_before * 100)
                fig = go.Figure(data=[go.Pie(values=[total_non_embedded_before, total_non_embedded_after],
                                             labels=['Before', 'After'], marker=dict(colors=['#FF6B6B', '#4ECDC4']),
                                             textinfo='label+value', textposition='outside',
                                             texttemplate='%{label}<br>%{value:.6f} gCO2eq', hole=0.7,
                                             direction='clockwise', showlegend=False)])
                fig.update_layout(title=dict(text='Non-Embedded Code Emissions (gCO2eq)<br>[".py", ".java", ".cpp", ".rb"]',
                                             y=0.95, x=0.5, xanchor='center', yanchor='top', font=dict(size=18)),
                                 annotations=[dict(text=f"↓{reduction_percentage:.1f}%", x=0.5, y=0.5,
                                                   font=dict(size=24, color='green'), showarrow=False),
                                             dict(text="Reduction", x=0.5, y=0.38, font=dict(size=14, color='green'),
                                                  showarrow=False)], width=600, height=400, paper_bgcolor='white',
                                 plot_bgcolor='white', showlegend=False)
                template_vars['div_pie_chart_non_embedded'] = pio.to_html(fig, include_plotlyjs=False, full_html=False)

        except Exception as e:
            logging.error(f"Error processing before/after data: {e}")

    # Render the templates with dynamic data
    try:
        html_content = template.render(**template_vars)
        html_details_content = details_template.render(
            solution_dirs=solution_dirs,  # Directly use the parameter
            before_details=template_vars.get('before_details', []),
            after_details=template_vars.get('after_details', []),
            detailed_data=detailed_data  # Directly use the parameter
        )
        timestamp_html_content = lastrun_template.render(
            latest_total_before=f"{template_vars.get('latest_total_before', 0.0):.2f}",
            latest_total_after=f"{template_vars.get('latest_total_after', 0.0):.2f}",
            latest_energy_table_html=template_vars.get('latest_energy_table_html', "<p>Data unavailable</p>"),
            latest_emissions_table_html=template_vars.get('latest_emissions_table_html', "<p>Data unavailable</p>"),
            latest_total_emissions_before=f"{template_vars.get('latest_total_emissions_before', 0.0):.2f}",
            latest_total_emissions_after=f"{template_vars.get('latest_total_emissions_after', 0.0):.2f}",
            last_run_timestamp=template_vars.get('last_run_timestamp', "No data"),
            unique_hosts=template_vars.get('unique_hosts', 0),
            average_co2_emission=round(template_vars.get('average_co2_emission', 0.0), 4),
            average_energy_consumption=round(template_vars.get('average_energy_consumption', 0.0), 4),
            average_cpu_usage=round(template_vars.get('average_cpu_usage', 0.0), 2),
            average_ram_usage=round(template_vars.get('average_ram_usage', 0.0), 2),
            average_disk_usage=round(template_vars.get('average_disk_usage', 0.0), 2),
            average_network_usage=round(template_vars.get('average_network_usage', 0.0), 2),
            cpu_usage_data=template_vars.get('cpu_usage_data', []),
            ram_usage_data=template_vars.get('ram_usage_data', []),
            disk_usage_data=template_vars.get('disk_usage_data', []),
            network_usage_data=template_vars.get('network_usage_data', []),
            disk_read_data=template_vars.get('disk_read_data', []),
            disk_write_data=template_vars.get('disk_write_data', []),
            max_network=round(template_vars.get('max_network', 0.0), 2),
            critical_servers=template_vars.get('critical_servers', None),
            div_combined_graph=template_vars.get('div_combined_graph', "<p>Energy consumption data unavailable</p>"),
            div_emissions_combined_graph=template_vars.get('div_emissions_combined_graph', "<p>Emissions data unavailable</p>"),
            div_pie_chart_non_embedded=template_vars.get('div_pie_chart_non_embedded', "<p>Non-embedded code data unavailable</p>"),
            div_pie_chart_embedded=template_vars.get('div_pie_chart_embedded', "<p>Embedded code data unavailable</p>"),
            final_overview_data=template_vars.get('final_overview_data', {})
        )
        timestamp_html_details_content = lastrun_details_template.render(
            solution_dirs=solution_dirs,  # Directly use the parameter
            latest_before_details=template_vars.get('latest_before_details', []),
            latest_after_details=template_vars.get('latest_after_details', []),
            detailed_data=detailed_data  # Directly use the parameter
        )
        server_details = details_server_template.render(
            unique_servers=template_vars.get('unique_servers', []),
            server_details=template_vars.get('server_details', [])
        )
        recommendations_detalis = recommendations_template.render(
            unique_dates=template_vars.get('unique_dates', []),
            recommendations_details=template_vars.get('recommendations_details', {}),
            final_overview_data=template_vars.get('final_overview_data', {})
        )
    except Exception as e:
        logging.error(f"Template rendering failed: {e}")
        return

    # Save the reports
    current_date = datetime.now().strftime('%Y-%m-%d')
    current_time = datetime.now().strftime('%H-%M')
    date_folder_path = os.path.join(REPORT_DIR, current_date)
    time_folder_path = os.path.join(date_folder_path, current_time)
    os.makedirs(time_folder_path, exist_ok=True)

    report_paths = {
        'details_report': os.path.join(time_folder_path, 'details_report.html'),
        'emissions_report': os.path.join(time_folder_path, 'emissions_report.html'),
        'server_report': os.path.join(time_folder_path, 'server_report.html'),
        'recommendations_report': os.path.join(time_folder_path, 'recommendations_report.html'),
        'main_report': os.path.join(REPORT_DIR, 'emissions_report.html'),
        'detailed_report': os.path.join(REPORT_DIR, 'details_report.html'),
        'server_main_report': os.path.join(REPORT_DIR, 'server_report.html'),
        'recommendations_main_report': os.path.join(REPORT_DIR, 'recommendations_report.html')
    }

    for name, path in report_paths.items():
        try:
            with open(path, 'w', encoding="utf-8") as f:
                if name == 'details_report':
                    f.write(timestamp_html_details_content)
                elif name == 'emissions_report':
                    f.write(timestamp_html_content)
                elif name == 'server_report':
                    f.write(server_details)
                elif name == 'recommendations_report':
                    f.write(recommendations_detalis)
                elif name == 'main_report':
                    f.write(html_content)
                elif name == 'detailed_report':
                    f.write(html_details_content)
                elif name == 'server_main_report':
                    f.write(server_details)
                elif name == 'recommendations_main_report':
                    f.write(recommendations_detalis)
            logging.info(f"Report generated at {path}")
        except Exception as e:
            logging.error(f"Failed to save report {name}: {e}")

# Generate HTML report
generate_html_report(RESULT_DIR, solution_dirs=solution_dirs, detailed_data=detailed_data)